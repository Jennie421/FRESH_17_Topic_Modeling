---
title: "Subject_level_topics"
author: "Jennie"
date: "7/21/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Subject transcripts

```{r, message=FALSE, include=FALSE}
library(tidyverse)
library(tm)               
library(slam)                       ## utility functions for sparse matrices 
library(wordcloud)
library(textreg)                    ## package with some utilities
library(readr)                      ## helpful for encoding
library(anacor)                     ## package for correspondence analysis 
library(dplyr)

library(quanteda)
```

## Establish Corpus (all subjects' all transcripts)
```{r}
#import CSV files into one data frame
cur_dir = getwd()
path = paste(cur_dir, '/../FRESH_17_text/subject_level_text/FRESH_17_subjectText.csv', sep='')
study_id = 'FRESH_17'
myTable = data.frame(read.csv(path))
myCorpus = VCorpus(DataframeSource(myTable)) # Second way of creating corpus
```


### Pre-processing
```{r, warning=FALSE}
## user-defined stopwords
myStopwords <- c("and", "but", "were", "are",
                 "those", "just", "like", "um", "ve", "ll", "re", "blah",
                 "redacted", "inaudible", "hmm", "thing", "things", "yada", "yeah",
                 "three", "five")

myCorpus <- tm_map(myCorpus, removeWords, c(stopwords("english"), myStopwords))  ## remove default and user-defined stopwords
myCorpus <- tm_map(myCorpus, removeNumbers)      ## remove numbers 
```

### Quick Word cloud viz on corpus
```{r}
## quick word cloud or entire corpus
set.seed(1113)
dev.new()
wordcloud(myCorpus, colors = brewer.pal(8, "Dark2"), min.freq = 10, scale = c(2, 0.5))
title(study_id)
```

## document-feature matrix then convert to DTM, Easier trim by frequency
```{r, eval=FALSE}
corp <- corpus(myTable, text_field = "text")
dfm <- dfm(corp)
dfm2 <- dfm_trim(dfm, min_termfreq = 0.2)
dfm3 <- dfm_remove(dfm2, myStopwords, verbose = TRUE)
dfm3 <- dfm_remove(dfm3, c(stopwords()), verbose = TRUE)
dtm <- convert(dfm3, to = "tm")
inspect(dtm)
```


## Establish DTM 
```{r}
dtm <- DocumentTermMatrix(myCorpus)        ## create dtm (tf-weighted)
dtm
as.matrix(dtm)[1:3, 1:8]                       ## show first 10 words   
dim(as.matrix(dtm))
```

## Filter out unimportant words
```{r}
tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
cut <- quantile(tfidf, probs = 0.20)       ## keep 80% most important words
dtm2 <- dtm[, tfidf >= cut]        ## subset tf weighted DTM

dtm2                                       ## obviously less words than in original DTM
dtm2_mat <- as.matrix(dtm2)                ## convert into matrix
rowSums(dtm2_mat)                              ## check row sums
## Important: it can happen that for some documents we have full 0 frequencies. 
## Those need to be eliminated before CA
dim(dtm2_mat)
dtm2_mat[, 30:38]
```


## Fit Correspondence Analysis 
```{r}
## --- fit CA
fitca <- anacor(dtm2)           ## fit 2D CA solution
fitca
```

```{r}
## plot rows (documents, contributions)
fitca$row.scores
plot(fitca, plot.type = "rowplot", main = "Transcripts")  
```


# Topic Modeling 

```{r}
library("topicmodels")              ## fitting topic models
DTmat2 = dtm2  # Use cleaned, established dtm2
```

## Finding a reasonable number of topics
```{r}
library("ldatuning")
set.seed(123)
Ktopics <- FindTopicsNumber(DTmat2, topics = 2:30, metrics = c("Arun2010", "CaoJuan2009", "Griffiths2004", "Deveaud2014"))
FindTopicsNumber_plot(Ktopics)
```

## fit topic model
```{r}
K <- 6
set.seed(123)
goplda = LDA(dtm2, k = K, method = "Gibbs")
goplda
```

### Posterior frequency 
```{r}
postprob <- posterior(goplda)
```


### Each word gets a full vector of probability according to the topic assignments.
```{r}
pterms <- as.data.frame(t(postprob$terms))
terms(goplda, 30)                              ## top 20 terms in each topic
```

### each individual/document is assigned to a certain topic.
```{r}
ptopics <- as.data.frame(t(postprob$topics))
save_path = paste(cur_dir, "/../outputs/ptopics.csv", sep='')
write.csv(ptopics, save_path) # Output the probability table for document assignment.
topics(goplda)[1:10]                           ## assignments of the first 10 voters to each topic
```

### visualization using word clouds
```{r}
set.seed(123)
w2 <- pterms %>% mutate(word = rownames(pterms)) %>% gather(topic, weight, -word)

n <- 90
pal <- rep(brewer.pal(9, "Greys"), each = ceiling(n/9))[n:1]
dev.new()
op <- par(mfrow = c(3,2), mar = c(3,0,2,0))
for (i in 1:K) {
  w3 <- w2 %>% dplyr::filter(topic == i) %>% arrange(desc(weight))
  with(w3[1:n, ], wordcloud(word, freq = weight, scale = c(1.5, 0.5), random.order = FALSE, ordered.colors = TRUE, colors = pal))
  title(paste(study_id, "topic", i))
}
par(op)
```
