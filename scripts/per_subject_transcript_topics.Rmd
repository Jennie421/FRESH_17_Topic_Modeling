---
title: "FRESH17 Audio Diary Transcripts Topic Modeling"
author: "Jennie Li"
date: "7/14/2022"
output: pdf_document
---

The script is adapted from Patrick Mair. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Transcript level text topic modeling -- LDA 

```{r, message=FALSE, include=FALSE}
library(tidyverse)
library(tm)               
library(slam)                       ## utility functions for sparse matrices 
library(wordcloud)
library(textreg)                    ## package with some utilities
library(readr)                      ## helpful for encoding
library(anacor)                     ## package for correspondence analysis 
library(dplyr)
```

## set subject id
```{r}
s_list = list("3EF35", "3KS75", "6YG55", "7EK65", 
              "7XP88", "7ZW55", "8QZ55", "8VY68",  
               "9RP78", "9SA35")

subject_id = s_list[3]
```


## Establish Corpus
```{r}
#import CSV files into one data frame
cur_dir = getwd()
path = paste(cur_dir, '/../FRESH_17_text/transcript_level_text/', 'FRESH_17_', subject_id, '_daily_text.csv', sep='')
myTable = data.frame(read.csv(path))
myCorpus = VCorpus(DataframeSource(myTable)) # Second way of creating corpus
```


## Pre-processing
```{r, warning=FALSE}
## user-defined stopwords
myStopwords <- c("and", "but", "were", "are",
                 "those", "just", "like", "um", "ve", "ll", "re", "blah",
                 "redacted", "inaudible", "hmm", "thing", "things", "yada", "yeah",
                 "three", "five")

myCorpus <- tm_map(myCorpus, removeWords, c(stopwords("english"), myStopwords))  ## remove default and user-defined stopwords
myCorpus <- tm_map(myCorpus, removeNumbers)      ## remove numbers 
# inspect(myCorpus)    ## pre-processed texts
```



## Quick Word cloud viz on corpus
```{r}
## quick word cloud or entire corpus
set.seed(1113)
dev.new(width = 1000, height = 1000, unit = "px")
wordcloud(myCorpus, colors = brewer.pal(8, "Dark2"), min.freq = 10, 
          scale=c(2,.5))
title(subject_id)
```

## Establish DTM
```{r}
dtm <- DocumentTermMatrix(myCorpus)        ## create dtm (tf-weighted)
dtm
as.matrix(dtm)[1:10, 1:8]                       ## show first 10 words   
dim(as.matrix(dtm))
```


## Filter by tf-idf (term frequency-inverse document frequency)
```{r}
tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
cut <- quantile(tfidf, probs = 0.10)       ## 90% most important words
dtm2 <- dtm[, tfidf >= cut]        ## subset tf weighted DTM

dtm2                                       ## obviously less words than in original DTM
dtm2_mat <- as.matrix(dtm2)                ## convert into matrix
rowSums(dtm2_mat)                              ## check row sums
## Important: it can happen that for some documents we have full 0 frequencies. 
## Those need to be eliminated before CA
dim(dtm2_mat)
```


## Topic Modeling by LDA

```{r}
library("topicmodels")              ## fitting topic models
DTmat2 = dtm2  # Use cleaned, established dtm2
```

### find number of topics
```{r, eval=FALSE}
library("ldatuning")
set.seed(123)
Ktopics <- FindTopicsNumber(DTmat2, topics = 2:30, metrics = c("Arun2010", "CaoJuan2009", "Griffiths2004", "Deveaud2014"))
FindTopicsNumber_plot(Ktopics)
```

### fit LDA model
```{r}
K <- 5
set.seed(123)
goplda = LDA(dtm2, k = K, method = "Gibbs")
goplda
```

### Posterior frequency 
```{r}
postprob <- posterior(goplda)
pterms <- as.data.frame(t(postprob$terms))
round(head(pterms, 60), 4)                    ## probabilistic assignments of words to clusters
# Each word gets a full vector of probability according to the topic assignments.
```


```{r}
ptopics <- as.data.frame(t(postprob$topics))
save_path = paste(cur_dir, "/../outputs/",subject_id,"_LDA_topics.csv", sep="")
write.csv(ptopics, save_path) # Output the probability table for document assignment.

terms(goplda, 10)                              ## top 10 terms in each topic
```


```{r}
## visualization using word clouds
set.seed(123)
w2 <- pterms %>% mutate(word = rownames(pterms)) %>% gather(topic, weight, -word)
print(w2)

n <- 90
pal <- rep(brewer.pal(9, "Greys"), each = ceiling(n/9))[n:1]
dev.new(width = 1000, height = 1000, unit = "px")
op <- par(mfrow = c(3,2), mar = c(3,0,2,0))
for (i in 1:K) {
  w3 <- w2 %>% dplyr::filter(topic == i) %>% arrange(desc(weight))
  with(w3[1:n, ], wordcloud(word, freq = weight, scale = c(1.5, 0.5), random.order = FALSE, ordered.colors = TRUE, colors = pal))
  title(paste(subject_id, "topic", i))
}
par(op)
```




================================================================================
# Vector space model - similarity between words/terms

## clean environment
```{r}
# rm(list = ls())
```

## libraries
```{r warning=FALSE, included=FALSE}
library(tidyverse)
library(tm)               
library(dplyr)
library(slam)                       ## utility functions for sparse matrices 

library("irlba")                    ## efficient SVD which we will use for LSA
library("MPsychoR")                 ## GOP statements
library("randomNames")              ## we add random names to make it a bit more sparkling
library("tm")
library("slam")
```


## Text Pre-processing, establish DTM
```{r}
#import CSV files into one data frame
cur_dir = getwd()
path = paste(cur_dir, '/../FRESH_17_text/transcript_level_text/', 'FRESH_17_', subject_id, '_daily_text.csv', sep='')
study_id = 'FRESH_17'
myTable = data.frame(read.csv(path))
myCorpus = VCorpus(DataframeSource(myTable)) # creating corpus 

## user-defined stopwords
myStopwords <- c("and", "but", "were", "are",
                 "those", "just", "like", "um", "ve", "ll", "re", "blah",
                 "redacted", "inaudible", "hmm", "thing", "things", "yada", "yeah",
                 "three", "five")

myCorpus <- tm_map(myCorpus, removeWords, c(stopwords("english"), myStopwords))  ## remove default and user-defined stopwords
myCorpus <- tm_map(myCorpus, removeNumbers)      ## remove numbers 

## Establish DTM 
dtm <- DocumentTermMatrix(myCorpus)        ## create dtm (tf-weighted)
```

## Filter out unimportant words
```{r}

tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
cut <- quantile(tfidf, probs = 0.03)       ## keep 90% most important words
dtm2 <- dtm[, tfidf >= cut]    #    ## subset tf weighted DTM
tdmat2 <- t(dtm2)
tdmat2
```


## Quick find association 
```{r}
all(col_sums(tdmat2) > 0)
ind <- which(col_sums(tdmat2)  > 0)         
tdmat2 <- tdmat2[,ind]                   ## remove Anthony (full 0 vector)
all(col_sums(tdmat2) > 0)                 ## check OK
tdmat2
findAssocs(tdmat2, c("parents", "stressful"), corlimit = c(0.5, 0.5))   ## quick association computation
## Computes Pearson correlations between e.g. the "gun" tf-idf vector and the remaining words, 
## and then prints out the ones that are above a particular threshold. 
```


## --- fit LSA 
```{r}
tdmat2 <- as.matrix(tdmat2)               ## convert into matrix
tdmat2

doc_names = colnames(tdmat2)
```


### Extract 50 dimensions 
```{r}
fit_lsa <- irlba(tdmat2, nv = 5)        ## efficient SVD computation (partial SVD)
## nv is the number of dimensions. We use 50 here, as this is a very small corpus. Usually something 
## like 300 is a good number. 
```


### Plot term vector space using U matrix, doc vector space using V matrix
```{r}
str(fit_lsa)              ## matrices, U, D, V
V <- fit_lsa$v            ## extract V 
dim(V)                    ## documents in rows, new variables in columns
U <- fit_lsa$u            ## terms in rows, new variables in columns
dim(U)
D <- diag(fit_lsa$d)
dim(D)

rownames(V) = doc_names # assign each document with the original name, i.e. acad_cal_day

save_path = paste(cur_dir, "/../outputs/",subject_id,"_LSA_topics.csv", sep="")
write.csv(V, save_path) # Output the probability table for document assignment.
```


## --- plot documents using MDS
```{r}
library(basicPlotteR)
cos_dist <- proxy::dist(V, "cosine") 
dim(cos_dist)
fit_mds <- cmdscale(cos_dist)

plot(fit_mds, pch = 19, cex = 0.1, xlab = "D1", ylab = "D2", main = "Document Configuration")
text(fit_mds, labels = colnames(tdmat2), cex = 0.6, pos = 3)

```


```{r}
library("lsa")
cosine(V[129, ], V[178, ])      ## similarity of new voter to first voter in sample
cosine(V[129, ], V[44, ])      ## similarity of new voter to second voter in sample, etc.
```


```{r, eval=FALSE}
rm(list = ls()); gc()            ## free up memory
```





